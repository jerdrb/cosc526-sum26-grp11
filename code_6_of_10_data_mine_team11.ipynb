{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee33933",
   "metadata": {},
   "source": [
    "# m.5.Assignment -> Spooky authorship identification via Apache Spark\n",
    "\n",
    "\n",
    "| **Overview** | Use Apache Spark and machine learning to determine sentence authorship labels. |\n",
    "|----------|--------------------------------------------------------------------------------|\n",
    "| **Data** | [Dark, ominous, and introspective](https://www.kaggle.com/competitions/spooky-author-identification/code) |\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "## **DETAILS**\n",
    "#### **Dataset Description:** \n",
    "The spooky author identification dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley. The data was prepared by chunking larger texts into sentences using CoreNLP's MaxEnt sentence tokenizer resulting in an odd non-sentence here and there. Your objective is to accurately identify the author of the sentences in the test set.\n",
    "- **id** - unique identifier for each sentence\n",
    "- **text** - sentence written by one of the authors\n",
    "- **author** - {EAP:Edgar Allan Poe}, {HPL:HP Lovecraft}; {MWS:Mary Wollstonecraft Shelley}\n",
    "#### **Objective:**\n",
    "- A. Accurately identify the author of the sentences in the test set.\n",
    "- B. Perform ALL work using Apache Spark.\n",
    "#### **Dataset:**\n",
    "- Training consists of passages with an author label.\n",
    "- Test has sentences with no author labels.\n",
    "#### **Competition Evaluation:**\n",
    "The submissions were evaluated based on multi-class logarithmic loss. The logarithmic loss assesses the uncertainty of the predicted probabilities, penalizing confident incorrect predictions. Lower log loss values indicated better performance. \n",
    "#### **Approach:**\n",
    "NLP techniques + machine learning algorithms. Feature engineering like bag-of-words, TF-IDF, word embeddings/Word2Vec. Perform algorithmic work with logistic regression, support vector machines, neural networks, and as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719b1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05301f28",
   "metadata": {},
   "source": [
    "## **TASKS**\n",
    "\n",
    "\n",
    "### **Stage 0: Import Data**\n",
    "\n",
    "1. Create a code notebook called: code_6_of_10_data_mine_<your_name>.ipynb\n",
    "2. Load data into Spark data objects and explore structure, size, and distribution of information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde93826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n",
      "\n",
      "+-------+--------------------+------+\n",
      "|     id|                text|author|\n",
      "+-------+--------------------+------+\n",
      "|id26305|This process, how...|   EAP|\n",
      "|id17569|It never once occ...|   HPL|\n",
      "|id11008|In his left hand ...|   EAP|\n",
      "+-------+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Test DataFrame:\n",
      "\n",
      "+-------+--------------------+\n",
      "|     id|                text|\n",
      "+-------+--------------------+\n",
      "|id02310|Still, as I urged...|\n",
      "|id24541|If a fire wanted ...|\n",
      "|id00134|And when they had...|\n",
      "+-------+--------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# The notebook already created.\n",
    "# Load data into Spark data objects and explore structure, size and distribution of information\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SpookyAuthor\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Load the train.csv & test.csv dataset into a Spark DataFrame\n",
    "train_df = spark.read.csv(\"data/train.csv\", header=True, inferSchema=True)\n",
    "test_df = spark.read.csv(\"data/test.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the train and test DataFrame\n",
    "print(\"Train DataFrame:\\n\")\n",
    "train_df.show(3)\n",
    "\n",
    "print(\"\\nTest DataFrame:\\n\")\n",
    "test_df.show(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
